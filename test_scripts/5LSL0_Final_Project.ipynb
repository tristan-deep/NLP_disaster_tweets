{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5LSL0_Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brLLc7DHoXtx",
        "colab_type": "text"
      },
      "source": [
        "Make a folder in your \"My Drive\" main folder of Google Drive named \"5LSL0_final\".<br>\n",
        "Put this colab notebook file inside that folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ8DecDfgjS1",
        "colab_type": "text"
      },
      "source": [
        "# Mount Google Drive\n",
        "Run the next cell to start working on the project!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt5M6wrAqZCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efmYI6dheXaV",
        "colab_type": "text"
      },
      "source": [
        "# Clone GitHub repo\n",
        "Run the next two cells to clone the GitHub repo. This has now already been done, so should not be necessary to do it again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d7bvEp_cGQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Go to the base folder in Google Drive\n",
        "%cd /content/gdrive/My\\ Drive/5LSL0_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBBREoa6Bat1",
        "colab": {}
      },
      "source": [
        "# Clone the GitHub repo\n",
        "# Only have to do this ones!\n",
        "!git clone https://github.com/tristan-deep/NLP_disaster_tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_kXso6Xd7CH",
        "colab_type": "text"
      },
      "source": [
        "# Pull changes from GitHub repo\n",
        "Run the next to cells before working on the project to sync the Google Drive folder with the GitHub repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7LY9KW1bnQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Go to the cloned repo folder\n",
        "%cd /content/gdrive/My\\ Drive/5LSL0_final/NLP_disaster_tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aV8S9ha_BZVS",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# git pull all changes that we made to the repo\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycz997ALt7kx",
        "colab_type": "text"
      },
      "source": [
        "# Try different things"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXU9Gm16erPz",
        "colab_type": "text"
      },
      "source": [
        "Go to working folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSkbkf3K-xOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/5LSL0_final/NLP_disaster_tweets')\n",
        "%pwd # print current location"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-BHIwyDmS5H",
        "colab_type": "text"
      },
      "source": [
        "This is the dataloader with some changes to make it work for Colab indicated with # COLAB CHANGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCkWdqeOWpAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## DataLoader\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.keras as keras\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def TokenizeTweets(vocabulary_size=10000):\n",
        "  data = pd.read_csv(Path('dataset', 'train' + '_set.csv'))\n",
        "  list_IDs = list(range(len(data)))\n",
        "  all_text = data.text.to_list()\n",
        "\n",
        "  # Create Tokenizer Object\n",
        "  tokenizer = Tokenizer(num_words=vocabulary_size, filters='#$%&()*+-<=>@[\\\\]^_`{|}~\\t\\n', lower=False, split=\" \")\n",
        "\n",
        "  # Train the tokenizer to the texts (training data)\n",
        "  tokenizer.fit_on_texts(all_text)\n",
        "\n",
        "  return tokenizer\n",
        "\n",
        "class LoadTweets(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "\n",
        "    def __init__(self, tokenizer, split, batch_size=32, n_classes=2, shuffle=True, vocabulary_size = 10000, max_length=500):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        self.split = split\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.data = pd.read_csv(Path('dataset', self.split + '_set.csv'))\n",
        "        self.all_text = self.data.text.to_list()\n",
        "        self.max_length = max_length\n",
        "        # Train the tokenizer to the texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # not using ids but rather rows in csv file for convenience\n",
        "        self.list_IDs = list(range(len(self.data)))\n",
        "\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples'\n",
        "        # Initialization\n",
        "\n",
        "        # Generate data, we can change this later when we know exactly\n",
        "        # in what kind of format we think data should be fed into network.\n",
        "        data = self.data.loc[list_IDs_temp, :]\n",
        "\n",
        "        X = {'keyword': data.keyword.to_list(),\n",
        "             'location': data.location.to_list(),\n",
        "             'text': data.text.to_list()}\n",
        "\n",
        "        X_text = X['text']\n",
        "        # Convert list of strings into list of lists of integers\n",
        "        X_sequences = self.tokenizer.texts_to_sequences(X_text)\n",
        "        # Truncate and pad input sequences\n",
        "        X_pad = sequence.pad_sequences(X_sequences, maxlen=self.max_length)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            y = data.target.to_list()\n",
        "            y = np.expand_dims(y, axis=1) # COLAB CHANGE: removed the square brackets\n",
        "            #y = keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
        "        elif self.split == 'val':\n",
        "            y = data.target.to_list()\n",
        "            y = np.expand_dims(y, axis=1) # COLAB CHANGE: removed the square brackets\n",
        "            # y = keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
        "        elif self.split == 'test':\n",
        "            # for test set there are no labels\n",
        "            y = None\n",
        "            \n",
        "        return X_pad, y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    vocabulary_size =10000\n",
        "    max_length = 500\n",
        "    tokenizer = TokenizeTweets(vocabulary_size)\n",
        "\n",
        "    gen = LoadTweets(tokenizer, split='train', batch_size=1, shuffle=False, vocabulary_size = vocabulary_size, max_length=max_length)\n",
        "\n",
        "    # example that prints all the tweets of the first batch\n",
        "    batch = gen[0]  # first of len(gen) batches\n",
        "\n",
        "    X = batch[0]  # 0 -> keywords/location/text, 1 -> target\n",
        "    text_first_batch = gen.tokenizer.sequences_to_texts(X)\n",
        "    print(text_first_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE3Uik-emL1w",
        "colab_type": "text"
      },
      "source": [
        "I copied the LSTM model here, to easily change the architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwx3OWSX0C12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Conv2D, LeakyReLU, AvgPool2D, UpSampling2D, ReLU, MaxPooling2D, Reshape, Softmax, Activation, Flatten, Lambda, Conv2DTranspose, Dropout, Embedding\n",
        "from tensorflow.keras.losses import MSE, categorical_crossentropy, binary_crossentropy\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model(max_words=10000, embedding_vecor_length=32, max_length=500, lstm_out=100, print_summary=True):\n",
        "    # create the model\n",
        "    # model = Sequential()\n",
        "    # model.add(Embedding(max_words, embedding_vecor_length, input_length=max_length))\n",
        "    # model.add(LSTM(lstm_out))\n",
        "    # model.add(Dense(1, activation='sigmoid'))\n",
        "    # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # if print_summary:\n",
        "    #     print(model.summary())\n",
        "        \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_words, embedding_vecor_length, input_length = max_length))\n",
        "    model.add(LSTM(lstm_out))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, name='out_layer'))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer='adam',\n",
        "                   metrics = ['accuracy'])\n",
        "    \n",
        "    if print_summary:\n",
        "        print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu9mHe3TnDo5",
        "colab_type": "text"
      },
      "source": [
        "Start the tensorboard application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMWwG3eWaHo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJlHurAem9uv",
        "colab_type": "text"
      },
      "source": [
        "Create model, train and save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G86gueWt_W3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from DataLoader import LoadTweets, TokenizeTweets\n",
        "# from models.LSTM_model import create_model\n",
        "\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "save_model = True\n",
        "model_name = 'lstm_embedding-100_out-100-epochs-15.h5'\n",
        "\n",
        "#Make a folder to save model weights, and\n",
        "run = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = Path(\"logs/\"+run)\n",
        "\n",
        "#check if the directory to save the model in exists\n",
        "os.makedirs(os.path.dirname(logdir), exist_ok=True)\n",
        "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
        "\n",
        "\n",
        "\"\"\"Create model\"\"\"\n",
        "vocabulary_size = 1000\n",
        "max_length = 100\n",
        "embedding_vector_length = 100\n",
        "lstm_out = 100\n",
        "\n",
        "tokenizer = TokenizeTweets(vocabulary_size=vocabulary_size)\n",
        "\n",
        "train_gen = LoadTweets(tokenizer, split='train',\n",
        "                        batch_size = 32, shuffle=True,\n",
        "                        vocabulary_size=vocabulary_size, max_length=max_length)\n",
        "val_gen = LoadTweets(tokenizer, split='val',\n",
        "                      batch_size = 32, shuffle=False,\n",
        "                      vocabulary_size=vocabulary_size, max_length=max_length)\n",
        "\n",
        "model = create_model(vocabulary_size, embedding_vector_length, max_length, lstm_out)\n",
        "\n",
        "\n",
        "\"\"\"## Train model\"\"\"\n",
        "model.fit(train_gen, validation_data = val_gen, epochs=1, callbacks = [tensorboard_callback])\n",
        "\n",
        "if save_model == True :\n",
        "    PATH = os.path.join('weights', model_name)\n",
        "    model.save(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb3n0FadnBny",
        "colab_type": "text"
      },
      "source": [
        "Show the tensorboard with the training logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjO6PzrxRmWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}